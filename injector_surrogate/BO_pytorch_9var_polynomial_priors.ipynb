{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#NN Surrogate model class\n",
    "from injector_surrogate_quads import *\n",
    "import physics_gp\n",
    "import os\n",
    "\n",
    "sys.path.append('../configs')\n",
    "#Sim reference point to optimize around\n",
    "from ref_config import ref_point\n",
    "\n",
    "#Pytorch \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "import botorch \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f51d051ae70>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BO with Expressive Priors - 2nd Order Polynomial\n",
    "### BO Minimizes Emittance*Bmag with 9 Variables (SQ, CQ, SOL, matching quads)"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load injector model\n",
    "Model = Surrogate_NN(pytorch=True)\n",
    "\n",
    "Model.load_saved_model(model_path = '../models/', \n",
    "                       model_name = 'Injector_Surrogate_NN_PyTorch')\n",
    "\n",
    "Model.load_scaling(scalerfilex = '../data/transformer_x_pytorch.pth', \n",
    "                   scalerfiley = '../data/transformer_y_pytorch.pth')\n",
    "Model.take_log_out = False"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-06-24 14:23:41.676866: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# design Twiss parameters \n",
    "beamline_info = json.load(open('../configs/beamline_info.json'))\n",
    "get_twiss0 = beamline_info['Twiss0']\n",
    "\n",
    "# emit, beta, alpha\n",
    "twiss0 = {'x': [get_twiss0[0], get_twiss0[2], get_twiss0[4]],\n",
    "          'y': [get_twiss0[1], get_twiss0[3], get_twiss0[5]]}\n",
    "\n",
    "beta0_x, alpha0_x = twiss0['x'][1], twiss0['x'][2]\n",
    "beta0_y, alpha0_y = twiss0['y'][1], twiss0['y'][2]\n",
    "# print(twiss0['x'])\n",
    "# print(twiss0['y'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objective Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert to machine units\n",
    "ref_point = Model.sim_to_machine(np.asarray(ref_point))\n",
    "\n",
    "# input params: solenoid and quads to vary \n",
    "opt_var_names = ['SOL1:solenoid_field_scale','CQ01:b1_gradient', 'SQ01:b1_gradient',\n",
    "                 \"QA01:b1_gradient\", \"QA02:b1_gradient\", \n",
    "                 \"QE01:b1_gradient\", \"QE02:b1_gradient\", \"QE03:b1_gradient\", \"QE04:b1_gradient\"]\n",
    "bounds = torch.as_tensor([[0.46, 0.485], [-0.02, 0.02], [-0.02, 0.02],\n",
    "                       [-4, -1], [1, 4],\n",
    "                       [-7,-1], [-1, 7],[-1, 7], [-7, 1]])\n",
    "\n",
    "# output params: emittance in transverse plane (x & y)\n",
    "opt_out_names = ['norm_emit_x','norm_emit_y']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(config): \n",
    "    \"\"\"\n",
    "    D is input space dimensionality\n",
    "    N is number of sample points\n",
    "    :param config: input values of opt_var_names, torch.tensor, shape (N, D) \n",
    "    returns (N, 1) \n",
    "    \"\"\"\n",
    "    N = config.shape[0]\n",
    "    D = config.shape[1]\n",
    "    \n",
    "    # make input array of length model_in_list (inputs model takes)\n",
    "    x_in = torch.empty((N,len(Model.model_in_list)))\n",
    "    \n",
    "    # fill in reference point around which to optimize\n",
    "    x_in[:,:] = torch.tensor(ref_point[0])\n",
    "\n",
    "    #set solenoid, CQ, SQ, matching quads to values from optimization step\n",
    "    col = []\n",
    "    for i in range(D):\n",
    "        col.append(Model.loc_in[opt_var_names[i]]) #should make col a flat list of indices, e.g. [4, 6, 7]\n",
    "    x_in[:, col] = config[:,:] \n",
    "    \n",
    "    #output predictions\n",
    "    y_out = Model.pred_machine_units(x_in)\n",
    "\n",
    "    return -1*objective(y_out)\n",
    "\n",
    "\n",
    "def objective(y_out):\n",
    "    \"\"\"\n",
    "    :param y_out: tensor with has a shape of (N, num_outputs)\n",
    "    returns tensor of emittance * bmag for each input, shape (N, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # geometric emittance in transverse plane\n",
    "    out1 = y_out[:,Model.loc_out['norm_emit_x']] #grab norm_emit_x out of the model\n",
    "    out2 = y_out[:,Model.loc_out['norm_emit_y']] #grab norm_emit_y out of the model\n",
    "    emit = torch.sqrt(out1 * out2)\n",
    "  \n",
    "    sigma_x = y_out[:,Model.loc_out['sigma_x']] #grab sigma_x out of the model \n",
    "    sigma_y = y_out[:,Model.loc_out['sigma_y']] #grab sigma_y out of the model \n",
    "    \n",
    "    # real beta and alpha \n",
    "    # NEEDS TO BE FIXED - currently assuming real alpha to be the same as design alpha \n",
    "    alpha_x = torch.tensor(alpha0_x).repeat(y_out.shape[0])\n",
    "    alpha_y = torch.tensor(alpha0_y).repeat(y_out.shape[0])\n",
    "    beta_x, beta_y = (sigma_x**2) / out1, (sigma_y**2) / out2\n",
    "    \n",
    "    # bmag \n",
    "    bmag_x = 0.5 * ((beta0_x / beta_x) + (beta_x / beta0_x)) + 0.5 * ((alpha_x * torch.sqrt(beta0_x / beta_x) - alpha0_x * torch.sqrt(beta_x / beta0_x))**2)\n",
    "    bmag_y = 0.5 * ((beta0_y / beta_y) + (beta_y / beta0_y)) + 0.5 * ((alpha_y * torch.sqrt(beta0_y / beta_y) - alpha0_y * torch.sqrt(beta_y / beta0_y))**2)\n",
    "    bmag = torch.sqrt(bmag_x * bmag_y)\n",
    "    \n",
    "    out = (emit * bmag)/1e-6 # in um units \n",
    "    return out.reshape(-1,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mesh grid of 3^9 points to sample\n",
    "n_samples_per_var = 3\n",
    "n_var = 9\n",
    "\n",
    "filename = f'grid{n_samples_per_var}^{n_var}.pt'\n",
    "\n",
    "\"\"\"\n",
    "create x and y if they are not saved in grid.pt, otherwise load x and y from grid.pt\n",
    "\"\"\"\n",
    "# create input and output data\n",
    "if os.path.exists('./results/'+filename) and os.stat('./results/'+filename).st_size > 0: \n",
    "    training_dict = torch.load('grid.pt')\n",
    "    x = training_dict['x']\n",
    "    y = training_dict['y']\n",
    "else: \n",
    "    var_points = torch.zeros((n_var, n_samples_per_var)) \n",
    "    # take n_samples_per_var points from each dimension i\n",
    "    for i in range(n_var):\n",
    "        var_points[i,:] = torch.linspace(bounds[i,0],bounds[i,1],n_samples_per_var)\n",
    "\n",
    "    # generate grid of points to sample \n",
    "    grid = np.array(np.meshgrid(*var_points)).reshape(n_var,-1)\n",
    "    x = (torch.tensor(grid).t())\n",
    "    x_data = torch.utils.data.TensorDataset(x)\n",
    "    x_loader = torch.utils.data.DataLoader(x_data, batch_size=3**9, shuffle=False)\n",
    "    \n",
    "    y = torch.cat([evaluate(x_sample[0]).detach() for batch_idx, x_sample in enumerate(x_loader)])\n",
    "    torch.save({'x': x, 'y': y}, './results/'+filename)\n",
    "\n",
    "print(x, y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.4600, -0.0200, -0.0200,  ..., -1.0000, -1.0000, -7.0000],\n",
      "        [ 0.4600, -0.0200, -0.0200,  ..., -1.0000, -1.0000, -3.0000],\n",
      "        [ 0.4600, -0.0200, -0.0200,  ..., -1.0000, -1.0000,  1.0000],\n",
      "        ...,\n",
      "        [ 0.4850,  0.0200,  0.0200,  ...,  7.0000,  7.0000, -7.0000],\n",
      "        [ 0.4850,  0.0200,  0.0200,  ...,  7.0000,  7.0000, -3.0000],\n",
      "        [ 0.4850,  0.0200,  0.0200,  ...,  7.0000,  7.0000,  1.0000]])\n",
      "torch.Size([19683, 9])\n",
      "tensor([[ -6.9780],\n",
      "        [-10.3164],\n",
      "        [ -5.1626],\n",
      "        ...,\n",
      "        [ -4.3536],\n",
      "        [ -6.9199],\n",
      "        [ -7.3886]], dtype=torch.float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit data with second order polynomial"
   ],
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# f(x) = x^T A x + B x + C\n",
    "# x is a 9x1 column vector, A is a symmetric 9x9 matrix, B is a 1x9 vector, C is a scalar.\n",
    "\n",
    "a_vals = torch.randn((int(n_var*(n_var+1)/2),1)).reshape(1,-1)\n",
    "A = torch.zeros(n_var, n_var)\n",
    "i, j = torch.triu_indices(n_var, n_var)\n",
    "A[i, j] = a_vals\n",
    "A.T[i, j] = a_vals\n",
    "A = A.clone().detach().requires_grad_(True)\n",
    "\n",
    "B = torch.randn((1, n_var), requires_grad=True)\n",
    "\n",
    "C = torch.randn((1,1), requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs):\n",
    "    # if (i > 30 and i % 10 == 0 and learning_rate >= 1e-7):\n",
    "    #     learning_rate /= 10\n",
    "    y_pred = torch.cat([torch.matmul(ele, torch.matmul(A, ele.T)) + torch.matmul(B, ele.T) + C for ele in x]).reshape(1,-1)[0]\n",
    "\n",
    "    loss = (y_pred - y).pow(2).mean() #\n",
    "    if (i % 50 == 0):\n",
    "        print(i, loss.item())\n",
    "        print(y_pred)\n",
    "    #print(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        A -= learning_rate * A.grad\n",
    "        B -= learning_rate * B.grad\n",
    "        C -= learning_rate * C.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        A.grad = None\n",
    "        B.grad = None\n",
    "        C.grad = None\n",
    "\n",
    "print(f'Result: y = x{A}X^T + {B} x + {C.item()}')\n",
    "torch.save({'x': x, 'y': y, 'A': A, 'B': B, 'C': C, 'y_pred': y_pred}, f'polynomial_{filename}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
      "         -0.7121],\n",
      "        [-0.7502,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
      "         -0.6970],\n",
      "        [-0.6540, -0.7773, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,\n",
      "          0.8073],\n",
      "        [-1.6095, -0.2515,  0.6995,  1.1017, -0.1759, -2.2456, -1.4465,  0.0612,\n",
      "         -1.2150],\n",
      "        [-0.1002, -0.2223,  0.1991, -0.1759,  0.7312,  1.1718, -0.9274,  0.5451,\n",
      "          0.0663],\n",
      "        [-0.6092,  1.6871,  0.8657, -2.2456,  1.1718, -0.4370,  0.7626,  1.1633,\n",
      "         -0.0091],\n",
      "        [-0.9798,  0.2284,  0.2444, -1.4465, -0.9274,  0.7626, -0.8425,  0.1374,\n",
      "          0.9386],\n",
      "        [-1.6091,  0.4676, -0.6629,  0.0612,  0.5451,  1.1633,  0.1374, -0.1860,\n",
      "         -0.6446],\n",
      "        [-0.7121, -0.6970,  0.8073, -1.2150,  0.0663, -0.0091,  0.9386, -0.6446,\n",
      "          1.5392]], requires_grad=True)\n",
      "0 20197.07253819996\n",
      "tensor([-111.2910, -137.2792, -114.0115,  ...,  -37.7810,  -73.0650,\n",
      "         -59.0932], grad_fn=<SelectBackward0>)\n",
      "50 699.1088871261485\n",
      "tensor([-33.0469, -33.2415, -16.5781,  ..., -23.5721, -35.9460, -31.4619],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "100 539.4311814409912\n",
      "tensor([-33.4387, -28.2453, -15.2882,  ..., -22.7174, -32.4726, -34.4641],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "150 465.7976547460983\n",
      "tensor([-28.9096, -22.9327, -13.7321,  ..., -20.9703, -27.6334, -31.0728],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "200 425.54223805604187\n",
      "tensor([-24.5460, -18.6254, -12.0778,  ..., -19.1234, -23.5783, -27.4062],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "250 402.22541112685946\n",
      "tensor([-21.0045, -15.3251, -10.5634,  ..., -17.4486, -20.4366, -24.3422],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "300 388.07834446148115\n",
      "tensor([-18.2434, -12.8485,  -9.2849,  ..., -16.0224, -18.0565, -21.9220],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "350 379.1156031912413\n",
      "tensor([-16.1151, -11.0056,  -8.2482,  ..., -14.8410, -16.2649, -20.0410],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# models  \n",
    "training_dict = torch.load(f'polynomial_{filename}')\n",
    "x = training_dict['x']\n",
    "y = training_dict['y']\n",
    "A = training_dict['A']\n",
    "B = training_dict['B'] \n",
    "C = training_dict['C'] \n",
    "y_pred = training_dict['y_pred']\n",
    "\n",
    "def polynomial_model(x):\n",
    "    \"\"\"\n",
    "    :param x: input values, tensor, shape (num_samples, num_var) \n",
    "    returns y_pred as tensor, shape (1, num_samples) \n",
    "    \"\"\" \n",
    "    return torch.cat([torch.matmul(ele, torch.matmul(A, ele.T)) + torch.matmul(B, ele.T) + C for ele in x.float()]).reshape(1,-1)[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# use ref point values for the parameters that are kept constant \n",
    "scan_ref_point = torch.cat([torch.as_tensor([ref_point[0][Model.loc_in[param_name]]]) for param_name in opt_var_names])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "# plot loss\n",
    "i = torch.arange(100)\n",
    "#plt.plot(i, train_losses[:100], label = \"train\")\n",
    "plt.plot(i, valid_losses, label = \"valid\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "def scan(model_param, num_scans, model_prior):\n",
    "    \"\"\"\n",
    "    scans values across a single parameter of the model, keeping others constant\n",
    "    :param model_param: name of the model param that is being scanned, string, size 1 \n",
    "    :param num_scans: number of values scanned for parameter \n",
    "    returns model_test_y, polynomial_test_y of all scanned values  \n",
    "    \"\"\" \n",
    "    param_index = opt_var_names.index(model_param)\n",
    "    test_x = scan_ref_point.repeat(num_scans, 1) \n",
    "    test_x[:,param_index] = torch.linspace(bounds[param_index, 0], bounds[param_index, 1], num_scans)\n",
    "    #print(test_x)\n",
    "    \n",
    "    model_test_y = torch.as_tensor([evaluate(ele) for ele in test_x])\n",
    "    prior_test_y = model_prior(test_x.float()) \n",
    "    #prior_test_y = model_prior(F.normalize(test_x, dim = 0).float()) \n",
    "    return model_test_y, prior_test_y, test_x\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,35)\n",
    "for i in range(len(opt_var_names)):\n",
    "    model_param = opt_var_names[i] \n",
    "    model_test_y, prior_test_y, test_x = scan(model_param, 30, NN_model) \n",
    "    plt.subplot(9, 2, i+1)\n",
    "    plt.xlabel(model_param)\n",
    "    plt.ylabel('emmitance*bmag')\n",
    "    plt.plot(test_x[:,opt_var_names.index(model_param)], model_test_y, label = \"model_y\")\n",
    "    plt.plot(test_x[:,opt_var_names.index(model_param)], prior_test_y.detach(), label = \"NN_y_100epochs_2hidden_tanh\")\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}