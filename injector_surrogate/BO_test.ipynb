{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed841451-2ad2-4a82-8b8d-e8cc8904736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN Surrogate model class\n",
    "import injector_surrogate_quads\n",
    "from injector_surrogate_quads import *\n",
    "import physics_gp\n",
    "\n",
    "sys.path.append('../configs')\n",
    "#Sim reference point to optimize around\n",
    "from ref_config import ref_point\n",
    "\n",
    "#Pytorch \n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import botorch \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9dd34-10e0-4915-accf-957a40d4f47b",
   "metadata": {},
   "source": [
    "# BO for Minimizing Emittance*Bmag with 9 Variables (SQ, CQ, SOL, matching quads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ef807c-db73-418f-adfb-66d40cc2124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 16:46:12.502186: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load injector model\n",
    "Model = Surrogate_NN()\n",
    "\n",
    "Model.load_saved_model(model_path = '../models/', \\\n",
    "                       model_name = 'model_OTR2_NA_rms_emit_elu_2021-07-27T19_54_57-07_00')\n",
    "Model.load_scaling()\n",
    "Model.take_log_out = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee602f2-7593-4ddf-b836-2e7ce051d196",
   "metadata": {},
   "source": [
    "## Import design Twiss parameters (OTR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e7b392-8239-41c9-adbe-1c1f234db566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-06, 1.113081026, -0.0689403587]\n",
      "[1e-06, 1.113021659, -0.07029489754]\n"
     ]
    }
   ],
   "source": [
    "beamline_info = json.load(open('../configs/beamline_info.json'))\n",
    "get_twiss0 = beamline_info['Twiss0']\n",
    "\n",
    "# emit, beta, alpha\n",
    "twiss0 = {'x': [get_twiss0[0], get_twiss0[2], get_twiss0[4]],\n",
    "          'y': [get_twiss0[1], get_twiss0[3], get_twiss0[5]]}\n",
    "\n",
    "beta0_x, alpha0_x = twiss0['x'][1], twiss0['x'][2]\n",
    "beta0_y, alpha0_y = twiss0['y'][1], twiss0['y'][2]\n",
    "print(twiss0['x'])\n",
    "print(twiss0['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d0ea5-fda2-4658-bcc0-0f89757ffeb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712a59e5-3b68-4c92-b4d8-fc7f122feec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to machine units\n",
    "ref_point = Model.sim_to_machine(np.asarray(ref_point))\n",
    "\n",
    "# input params: solenoid and quads to vary \n",
    "opt_var_names = ['SOL1:solenoid_field_scale','CQ01:b1_gradient', 'SQ01:b1_gradient',\n",
    "                 \"QA01:b1_gradient\", \"QA02:b1_gradient\", \n",
    "                 \"QE01:b1_gradient\", \"QE02:b1_gradient\", \"QE03:b1_gradient\", \"QE04:b1_gradient\"]\n",
    "bounds = torch.tensor([[0.46, 0.485], [-0.02, 0.02], [-0.02, 0.02],\n",
    "                       [-4, -1], [1, 4],\n",
    "                       [-7,-1], [-1, 7],[-1, 7], [-7, 1]])\n",
    "\n",
    "# output params: emittance in transverse plane (x & y)\n",
    "opt_out_names = ['norm_emit_x','norm_emit_y']\n",
    "\n",
    "def evaluate(config): \n",
    "    \"\"\"\n",
    "    D is input space dimensionality\n",
    "    :param config: input values of opt_var_names, torch.tensor, shape (1, D) \n",
    "    \"\"\"\n",
    "    #make input array of length model_in_list (inputs model takes)\n",
    "    x_in = np.empty((1,len(Model.model_in_list)))\n",
    "\n",
    "    #fill in reference point around which to optimize\n",
    "    x_in[:,:] = np.asarray(ref_point[0])\n",
    "\n",
    "    #set solenoid, CQ, SQ, matching quads to values from optimization step\n",
    "    for i in range(config.size(dim=0)):\n",
    "        x_in[:, Model.loc_in[opt_var_names[i]]] = config[i]\n",
    "\n",
    "    #output predictions\n",
    "    y_out = Model.pred_machine_units(x_in) \n",
    "\n",
    "    return -1*objective(y_out)[0]\n",
    "\n",
    "\n",
    "def objective(y_out):\n",
    "    # output is emittance * bmag \n",
    "    \n",
    "    # geometric emittance in transverse plane\n",
    "    out1 = y_out[:, Model.loc_out['norm_emit_x']] #grab norm_emit_x out of the model\n",
    "    out2 = y_out[:, Model.loc_out['norm_emit_y']] #grab norm_emit_y out of the model\n",
    "    emit = np.sqrt(out1 * out2)\n",
    "  \n",
    "    sigma_x = y_out[:, Model.loc_out['sigma_x']] #grab sigma_x out of the model \n",
    "    sigma_y = y_out[:, Model.loc_out['sigma_y']] #grab sigma_y out of the model \n",
    "    \n",
    "    # real beta and alpha \n",
    "    # NEEDS TO BE FIXED - currently assuming real alpha to be the same as design alpha \n",
    "    alpha_x, alpha_y = alpha0_x, alpha0_y\n",
    "    beta_x, beta_y = (sigma_x**2) / out1, (sigma_y**2) / out2\n",
    "    \n",
    "    # bmag \n",
    "    bmag_x = 0.5 * ((beta0_x / beta_x) + (beta_x / beta0_x)) + 0.5 * ((alpha_x * np.sqrt(beta0_x / beta_x) - alpha0_x * np.sqrt(beta_x / beta0_x))**2)\n",
    "    bmag_y = 0.5 * ((beta0_y / beta_y) + (beta_y / beta0_y)) + 0.5 * ((alpha_y * np.sqrt(beta0_y / beta_y) - alpha0_y * np.sqrt(beta_y / beta0_y))**2)\n",
    "    bmag = np.sqrt(bmag_x * bmag_y)\n",
    "    #print(f'bmag: {bmag} emit: {emit}') \n",
    "    \n",
    "    return (emit * bmag)/1e-6 # in um units \n",
    "    #return np.sqrt(out1*out2)/1e-6 # in um units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95895f93-be9f-4fd1-b50c-ba4d488269bb",
   "metadata": {},
   "source": [
    "## Gaussian Regression & Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3579279b-21fd-4afc-b7b3-33b0fd38ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BO_point(x, f, bounds, beta=2.5, mean_module=None, input_transform=None, outcome_transform=None, precision = None, phys=False):\n",
    "    \"\"\"\n",
    "    function that trains a GP model of data and returns the next observation point using UCB\n",
    "    D is input space dimensionality\n",
    "    N is number of samples\n",
    "\n",
    "    :param x: input points data, torch.tensor, shape (N,D)\n",
    "    :param f: output point data, torch.tensor, shape (N,1)\n",
    "    :param bounds: input space bounds, torch.tensor, shape (2,D)\n",
    "    :param precision: precision matrix used for RBF kernel (must be PSD), torch.tensor, (D,D)\n",
    "    :param beta: UCB optimization parameter, float\n",
    "    :return x_candidate, model: next observation point and gp model w/observations\n",
    "    \"\"\"\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if phys == True:\n",
    "        gp = physics_gp.PhysicsExactGPModel(x, f.flatten(), likelihood, precision)\n",
    "    else:\n",
    "        gp = botorch.models.SingleTaskGP(x, f, likelihood=likelihood, \n",
    "                                         outcome_transform=outcome_transform, \n",
    "                                         input_transform=input_transform)\n",
    "        \n",
    "    # mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    \n",
    "    # fit GP hyperparameters\n",
    "    # botorch.fit.fit_gpytorch_model(mll)\n",
    "\n",
    "    # do UCB acquisition\n",
    "    UCB = botorch.acquisition.UpperConfidenceBound(gp, beta=beta)\n",
    "    candidate, acq_value = botorch.optim.optimize_acqf(UCB,\n",
    "                                                       bounds=bounds,\n",
    "                                                       q=1,\n",
    "                                                       num_restarts=20,\n",
    "                                                       raw_samples=20)\n",
    "    return candidate, gp, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab76dc5-7177-4a05-8133-4639cf66ae38",
   "metadata": {},
   "source": [
    "## Set up initial training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8e2cc5-715b-415d-a9c4-3d92e183d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.6111e-01, -1.9024e-03, -1.1049e-02, -1.6129e+00,  3.6219e+00,\n",
      "         -2.2369e+00, -1.1919e-01,  6.9243e-01, -2.2148e+00],\n",
      "        [ 4.7138e-01,  1.1747e-02, -1.8385e-02, -1.9002e+00,  3.8931e+00,\n",
      "         -1.9906e+00,  1.7254e-01,  2.9522e+00,  2.8048e-01],\n",
      "        [ 4.7673e-01, -1.8019e-02,  9.9461e-03, -1.7159e+00,  3.9872e+00,\n",
      "         -3.6145e+00,  3.3282e+00,  2.2762e+00, -2.6969e+00]])\n",
      "tensor([[-2.9347],\n",
      "        [-1.9276],\n",
      "        [-1.0459]], dtype=torch.float64)\n",
      "tensor([[0.0000],\n",
      "        [0.5332],\n",
      "        [1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#create initial samples within specified bounds\n",
    "n_samples = 3\n",
    "n_var = 9\n",
    "\n",
    "train_x = torch.zeros((n_samples, n_var)) \n",
    "for i in range(n_var):\n",
    "    train_x[:,i] = torch.as_tensor(np.random.uniform(bounds[i,0],bounds[i,1],(n_samples,)))\n",
    "print(train_x)\n",
    "\n",
    "train_y = torch.tensor([evaluate(vars) for vars in train_x]).reshape(-1,1)\n",
    "print(train_y)\n",
    "\n",
    "#transformer \n",
    "transformer_x = botorch.models.transforms.input.Normalize(n_var, bounds = bounds.transpose(0,1))\n",
    "transformer_y = botorch.models.transforms.outcome.Standardize(1)\n",
    "transformer_y = botorch.models.transforms.input.Normalize(1)\n",
    "print(transformer_y(train_y))\n",
    "\n",
    "normed_bounds = torch.cat((torch.zeros(1,n_var), torch.ones(1,n_var)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7ebe7-3652-467a-aabd-9c285ecc7dde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f08ef8-0cde-4c86-8e5b-57c8ac264e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesianOptimization(train_x, train_y, n_steps, prior = None, phys = False):\n",
    "    if (prior == None):\n",
    "        transformer_y = botorch.models.transforms.outcome.Standardize(1)\n",
    "    else:\n",
    "        transformer_y = botorch.models.transforms.input.Normalize(1)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        x_new, model, likelihood = get_BO_point(train_x, train_y, \n",
    "                                    bounds=bounds.transpose(0,1), \n",
    "                                    mean_module=prior, \n",
    "                                    input_transform=transformer_x, \n",
    "                                    outcome_transform=transformer_y, \n",
    "                                    phys=phys)\n",
    "\n",
    "        train_x = torch.cat((train_x, x_new))\n",
    "        new_y = torch.tensor(evaluate(train_x[-1])).reshape(1,1)\n",
    "        train_y = torch.cat((train_y, new_y))\n",
    "        \n",
    "        # print(\"iter     target       SOL        CQ        SQ        QA1        QA2        Q1        Q2        Q3        Q4\")\n",
    "        # print(f'{color[0]}{i+1}      {train_y[-1,0]:.5f}   {train_x[-1,0]:.5f}   {train_x[-1,1]:.5f}   {train_x[-1,2]:.5f}   {train_x[-1,3]:.5f}   {train_x[-1,4]:.5f}   {train_x[-1,5]:.5f}   {train_x[-1,6]:.5f}   {train_x[-1,7]:.5f}   {train_x[-1,8]:.5f}{color[1]}')\n",
    "        \n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee62231-5be3-4d31-9b57-b4381e23187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant prior mean\n",
    "constant_prior_best_y = BayesianOptimization(train_x, train_y, 1, prior = None, phys = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2474164-6fb1-4891-be6a-d74a57d65a3a",
   "metadata": {},
   "source": [
    "### use NN as prior mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9a950e-7da0-4c8c-8f01-42becd03cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "import torch.nn as nn\n",
    "class NN_prior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_prior, self).__init__()\n",
    "        \n",
    "        hidden_size = 20\n",
    "        self.network = nn.Sequential(nn.Linear(n_var, hidden_size), \n",
    "                                     nn.Tanh(), \n",
    "                                     nn.Linear(hidden_size, hidden_size), \n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x \n",
    "\n",
    "# create prior mean\n",
    "from gpytorch.means.mean import Mean\n",
    "class CustomMean(Mean):\n",
    "    def __init__(self, name):\n",
    "        super(CustomMean, self).__init__()\n",
    "        self.NN_model = NN_prior()\n",
    "        self.NN_model.load_state_dict(torch.load('./results/' + name + '.pth'))\n",
    "        self.NN_model.eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.NN_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ebbc21f-d06c-444a-a1e8-c1cf3bae9298",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105622/774050561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNN_prior_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomMean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_1hidden_20nodes_500epoch_0.02'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_prior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_105622/704991006.py\u001b[0m in \u001b[0;36mBayesianOptimization\u001b[0;34m(train_x, train_y, n_steps, prior, phys)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                     \u001b[0minput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     \u001b[0moutcome_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                     phys=phys)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_105622/2832317238.py\u001b[0m in \u001b[0;36mget_BO_point\u001b[0;34m(x, f, bounds, beta, mean_module, input_transform, outcome_transform, precision, phys)\u001b[0m\n\u001b[1;32m     20\u001b[0m         gp = botorch.models.SingleTaskGP(x, f, likelihood=likelihood, \n\u001b[1;32m     21\u001b[0m                                          \u001b[0moutcome_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutcome_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                          input_transform=input_transform)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baxenv/lib/python3.7/site-packages/botorch/models/gp_regression.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_X, train_Y, likelihood, covar_module, mean_module, outcome_transform, input_transform)\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutcome_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutcome_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tensor_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformed_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mignore_X_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_ignore_X_dims_scaling_check\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "NN_prior_mean = CustomMean('model_1hidden_20nodes_500epoch_0.02')\n",
    "model, likelihood = BayesianOptimization(train_x, train_y, 1, prior = NN_prior_mean, phys = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29038956-b897-471a-a983-371d66a72efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6448337-8ec1-48f0-bbae-5cd1106dbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#define acquisition function\n",
    "from botorch.acquisition.analytic import UpperConfidenceBound, ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "#optimize\n",
    "n_steps = 45\n",
    "for i in range(n_steps):\n",
    "    #best_normed_y = torch.max(normed_train_y)\n",
    "    UCB = UpperConfidenceBound(gp, beta=2.5)\n",
    "    #EI = ExpectedImprovement(gp, best_normed_y)\n",
    "\n",
    "    bounds = torch.cat((torch.zeros(1,3), torch.ones(1,3)), 0)\n",
    "    candidate, acq_value = optimize_acqf(UCB, bounds = bounds, num_restarts = 20, q = 1, raw_samples = 20)\n",
    "\n",
    "    train_x = torch.cat((train_x, transformer_x.backward(candidate)))\n",
    "    normed_train_x = transformer_x.forward(train_x)\n",
    "\n",
    "    new_y = torch.tensor(evaluate(train_x[-1][0], train_x[-1][1], train_x[-1][2])).reshape(1,1)\n",
    "    train_y = torch.cat((train_y, new_y))\n",
    "    \n",
    "    print(\"iteration        target         varx         vary         varz\")\n",
    "    print(f'{i+1}              {train_y[-1][0]:.5f}      {train_x[-1][0]:.5f}      {train_x[-1][1]:.5f}      {train_x[-1][2]:.5f}')\n",
    "    print(torch.max(train_y))\n",
    "    \n",
    "    transformer_y = transformer.Transformer(train_y, 'standardize')\n",
    "    normed_train_y = transformer_y.forward(train_y)\n",
    "\n",
    "    gp = SingleTaskGP(normed_train_x, normed_train_y)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_model(mll);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfb5ca-1288-468e-b19d-8360a35c94a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
